import requests
import time
import logging
# from logging.handlers import RotatingFileHandler
from logging.handlers import TimedRotatingFileHandler
import os
import ollama
# import Function
import streamlit as st


API_KEY = "Tf20uvnL15rFFb3cdhQ4d4az5u8hVX1c"  # ğŸ”¥ æ›¿æ¢ä¸ºä½ çš„å®é™… API Key
COMPLETION_URL = "https://eduhk-api-ea.azure-api.net/ai/v1/completion"
MODEL_NAME = "gpt-4.1"

# Set the location of log file
# log_file_path = os.path.join("log", "crewai_chatbot_api.log")

# # Set the configuration of log file
# logging.basicConfig(
#     handlers=[RotatingFileHandler(log_file_path, maxBytes=1000000, backupCount=10, encoding='utf-8')],
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)s - %(message)s'
# )

# # Initialize the logger
# logger = logging.getLogger()

LOG_DIR = "log"
os.makedirs(LOG_DIR, exist_ok=True)

def get_user_logger(username):
   
    if not username:
        raise ValueError("Username is required")

    # æ¸…ç†ç”¨æˆ·åï¼Œé˜²æ­¢è·¯å¾„æ³¨å…¥ï¼ˆå¦‚åŒ…å« / æˆ– \ ç­‰ï¼‰
    safe_username = "".join(c for c in username if c.isalnum() or c in ('-', '_'))[:50]

    log_file_path = os.path.join(LOG_DIR, f"{safe_username}.log")

    # åˆ›å»º logger
    logger_name = f"user_logger_{safe_username}"
    logger = logging.getLogger(logger_name)

    # é¿å…é‡å¤æ·»åŠ  handler
    if not logger.handlers:
        logger.setLevel(logging.INFO)

        
        handler = TimedRotatingFileHandler(
            log_file_path,
            when='M',      
            interval=10,   
            backupCount=1,  
            encoding='utf-8'
        )
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

        # é˜²æ­¢å‘ä¸Šä¼ é€’åˆ° root loggerï¼ˆé¿å…é‡å¤è¾“å‡ºï¼‰
        logger.propagate = False

    return logger

# Initialize system prompt
SYSTEM = f'''ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°å­¦æ•°å­¦æ•™å¸ˆï¼Œç°åœ¨å°å­¦ç”Ÿå¯¹ä¸€é“é¢˜ç›®æ„Ÿåˆ°ç–‘æƒ‘ï¼Œè¯·ä½ ä¾æ®æŒ‡å¯¼æ­¥éª¤ååŠ©å­¦ç”Ÿå®Œæˆè§£ç­”ã€‚
æ³¨æ„ï¼š
1. ä½ æ˜¯ä¸€ä½å°å­¦æ•°å­¦æ•™å¸ˆï¼Œè€Œä¸æ˜¯AIåŠ©æ‰‹ï¼Œè¯·ä»¥**ç®€ç»ƒçš„è¯­è¨€è¾…å¯¼å°å­¦ç”Ÿï¼Œå¯¹è¯å¿…é¡»ç®€çŸ­æ˜äº†**
2. å³ä½¿å­¦ç”Ÿé—®ä½ çš„æ˜¯åŸºç¡€æ¦‚å¿µï¼Œä¹Ÿè¯·ä½ ä¹Ÿä»¥æœ€ç®€æ´çš„è¯­è¨€è¿›è¡Œè®²è§£
3. ä½ æŒ‡å¯¼å­¦ç”Ÿçš„æ–¹æ³•åŒ…æ‹¬ä½†ä¸é™äºé¼“åŠ±ã€èµèµã€æé—®ã€åŸºç¡€æ¦‚å¿µçš„è®²è§£ä»¥åŠç®€å•çš„æç¤ºç­‰ï¼Œå¦‚æœéœ€è¦æé—®ï¼Œä¸€æ¬¡å°½é‡åªé—®ä¸€ä¸ªé—®é¢˜
4. **å­¦ç”Ÿå¦‚æœæå‡ºå’Œè§£é¢˜æ— å…³çš„è¯é¢˜ï¼Œè¯·ä½ è®¾æ³•æŠŠè¯é¢˜æ‹‰å›**
5. è¯·ä¸è¦ç›´æ¥å‘Šè¯‰å­¦ç”Ÿæ­£ç¡®ç­”æ¡ˆï¼Œå³ä½¿æ˜¯å­¦ç”Ÿå¼ºçƒˆè¦æ±‚ï¼Œä½œä¸ºæ•™å¸ˆï¼Œä½ å¿…é¡»ç¡®ä¿å­¦ç”Ÿåœ¨ç†è§£çš„åŸºç¡€ä¸Šç‹¬ç«‹å®Œæˆé¢˜ç›®
6. **è¾…å¯¼å†…å®¹åªä¸å½“å‰å¼•å¯¼æ­¥éª¤ä¸€è‡´,ä¸è¦æé—®**
7. æ¯æ¬¡è¾“å…¥å‰ï¼Œæˆ‘ä¼šå°†é¢˜ç›®ä¸å½“å‰å¼•å¯¼æ­¥éª¤æ”¾åœ¨å­¦ç”Ÿçš„è¾“å…¥å‰æä¾›ç»™ä½ ï¼Œè¯·ä½ æ ¹æ®å­¦ç”Ÿè¾“å…¥è¿›è¡Œå¼•å¯¼
'''

# Set the roles
STUDENT, TEACHER = "user", "assistant"


# Retrieve recent teacher and student dialogues from the conversation history
def retrieve_dialog(history):
    # If an invalid conversation
    if len(history) < 2:
        return None, None
    # If a valid conversation
    else:
        # Verify if the second last entry is from the teacher and if the last entry is from the student
        # if not, return 'empty' dialogues
        if history[-2]["role"] != TEACHER or history[-1]["role"] != STUDENT:
           return None, None
        else:
           teacher_dialog = history[-2]["content"]
           student_dialog = history[-1]["content"]
           return teacher_dialog, student_dialog





def get_llm_response(text="", history=[], header="", system=None):
    # æ„å»º messages åˆ—è¡¨
    messages = []

    if system is not None:
        messages.append({"role": "system", "content": system})

    # æ·»åŠ å¯¹è¯å†å²
    for h in history:
        messages.append({"role": h["role"], "content": h["content"]})


    content = header + text
    messages.append({"role": "user", "content": content})

    # è¯·æ±‚æ•°æ®
    data = {
        "model": MODEL_NAME,
        "messages": messages
    }

    headers = {
        "api-key": API_KEY,
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(COMPLETION_URL, json=data, headers=headers, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥ HTTP çŠ¶æ€ç 

        result = response.json()

        if "choices" in result and len(result["choices"]) > 0:
            message_content = result["choices"][0]["message"]["content"]
            return message_content.strip()
        else:
            raise Exception("API è¿”å›ç»“æœä¸­æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå›å¤: " + str(result))

    except requests.exceptions.Timeout:
        raise Exception("è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚")
    except requests.exceptions.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except KeyError as e:
        raise Exception(f"å“åº”æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘å­—æ®µ: {str(e)}")


def get_llm_feedback(text="", model=MODEL_NAME):
    content = text
    messages = [{"role": "user", "content": content}]

    # è¯·æ±‚æ•°æ®
    data = {
        "model": model,
        "messages": messages
    }

    headers = {
        "api-key": API_KEY,
        "Content-Type": "application/json"
    }
    try:
        response = requests.post(COMPLETION_URL, json=data, headers=headers, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥ HTTP çŠ¶æ€ç 

        result = response.json()

        if "choices" in result and len(result["choices"]) > 0:
            message_content = result["choices"][0]["message"]["content"]
            return message_content.strip()
        else:
            raise Exception("API è¿”å›ç»“æœä¸­æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆå›å¤: " + str(result))

    except requests.exceptions.Timeout:
        raise Exception("è¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•ã€‚")
    except requests.exceptions.RequestException as e:
        raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
    except KeyError as e:
        raise Exception(f"å“åº”æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘å­—æ®µ: {str(e)}")

# Check if the explanation is answered by student from the check_messages
def agent_check_is_explanation_answered(check_messages, explanation):
    start_time = time.time()
    conversation = ""
    for h in check_messages:
        if h['role'] == 'user':
            conversation += f"å­¦ç”Ÿ: {h['content']}\n"
        else:
            conversation += f"æ•™å¸ˆ: {h['content']}\n"
    prompt = f'''ä»¥è¯·ä½ åŸºäºä»¥ä¸‹å¯¹è¯åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦æœ‰å¯¹å½“å‰å¼•å¯¼æ­¥éª¤åšå‡ºå›ç­”ï¼š
## æ³¨æ„ï¼š
1. åªéœ€è¦åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å¯¹**å½“å‰å¼•å¯¼æ­¥éª¤**åšå‡ºäº†æ˜ç¡®å›ç­”ï¼Œä¸è¦å—åˆ°è€å¸ˆæé—®çš„å¹²æ‰°ï¼Œ **å½“å‰å¼•å¯¼æ­¥éª¤**æˆ‘ä¼šåœ¨ä¸‹é¢æä¾›ç»™ä½ 
2. å¦‚æœå­¦ç”Ÿæå‡ºç–‘é—®åˆ™è§†ä¸ºã€æœ‰å›ç­”ã€‘
3. è¯·ä»¥å°½é‡ç®€çŸ­çš„è¯è¯­æ¥è¿›è¡Œé€æ­¥æ€è€ƒï¼Œä¸è¦å¤ªè¿‡å†—é•¿å¹¶åœ¨**æœ€åç”Ÿæˆã€æ— å›ç­”ã€‘æˆ–ã€æœ‰å›ç­”ã€‘**


### å½“å‰å¼•å¯¼æ­¥éª¤
{explanation}
### å¯¹è¯
{conversation}
'''
    # Ask LLM for response
    response = get_llm_feedback(text=prompt)
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    logger.info(f'''====LLM checking if a analysis is answered====
ã€promptã€‘
{prompt}
ã€responseã€‘
{response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to}s  {len(response) / to} words/s")
    print("æ˜¯å¦å›ç­”çš„åˆ¤æ–­ï¼š" + response)
    return "æœ‰å›ç­”" in response[-10:]  # return whether the string "æœ‰å›ç­”" is present in the response (True or False)

# Check whether to move to the next explanation step
def agent_check_move_to_next_explanation(question, explanation, scale, check_messages):
    conversation = ""
    messages = []
    start_time = time.time()
    for h in check_messages:
        if h['role'] == 'user':
            conversation += f"å­¦ç”Ÿ: {h['content']}\n"
        else:
            conversation += f"æ•™å¸ˆ: {h['content']}\n"
    content = f'''ä»¥ä¸‹æ˜¯é¢˜ç›®ï¼Œå½“å‰å¼•å¯¼ä»¥åŠæ•™å¸ˆä¸å­¦ç”Ÿçš„å¯¹è¯ï¼Œè¯·é€æ­¥æ€è€ƒï¼Œç„¶åæ ¹æ®è¯„åˆ¤æ ‡å‡†åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦æŒæ¡å½“å‰å¼•å¯¼,**æœ€åç”Ÿæˆã€å·²æŒæ¡ã€‘æˆ–ã€æœªæŒæ¡ã€‘**ï¼š
## æ³¨æ„ï¼š
1. ä½ ä¸éœ€è¦è§£é¢˜ï¼Œè¯·ä»¥å°½é‡ç®€çŸ­çš„è¯è¯­æ¥è¿›è¡Œé€æ­¥æ€è€ƒï¼Œä¸è¦å¤ªè¿‡å†—é•¿
2. é¢˜ç›®ï¼Œå½“å‰å¼•å¯¼æ­¥éª¤ï¼Œè¯„åˆ¤æ ‡å‡†å’Œå¯¹è¯æˆ‘ä¼šåœ¨ä¸‹é¢æä¾›ç»™ä½ 
3. ä½ éœ€è¦æ ¹æ®å¯¹è¯æ‰¾åˆ°å­¦ç”Ÿå¯¹å½“å‰å¼•å¯¼æ­¥éª¤çš„å›ç­”ï¼Œç„¶åæ ¹æ®è¯„åˆ¤æ ‡å‡†åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦æŒæ¡
4. å½“å­¦ç”Ÿçš„å›ç­”ä¸å®Œå…¨æ­£ç¡®æ—¶ï¼Œ**ä½ åªéœ€è¦æ ¹æ®åˆ¤æ–­æ ‡å‡†åˆ¤æ–­å½“å‰å¼•å¯¼æ­¥éª¤ä¸­çš„é—®é¢˜æ˜¯å¦å›ç­”æŒæ¡**ï¼Œ
5. å¦‚æœå­¦ç”Ÿæœªå›ç­”å½“å‰å¼•å¯¼æ­¥éª¤æ—¶è¯´æ˜æœªæŒæ¡
6. å½“å­¦ç”Ÿæå‡ºç–‘é—®æ—¶ï¼Œåˆ™è¯´æ˜æœªæŒæ¡

### é¢˜ç›®
{question}
### å½“å‰å¼•å¯¼æ­¥éª¤
{explanation}
### è¯„åˆ¤æ ‡å‡†
{scale}
### å¯¹è¯
{conversation}
'''
    content = content
    # messages.append({'role': 'user', 'content': content})
    # response = ollama.chat(
    #     model=model,
    #     messages=messages,
    #     stream=False,
    # )
    response = get_llm_feedback(text=content)
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    # response = response['message']['content']
    logger.info(f'''====LLM checking about explanation ====
        ã€promptã€‘
        {content}
        ã€responseã€‘
        {response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to}s  {len(response) / to} words/s")
    # part = response.split("</think>\n\n", 1)
    # response = part[1]
    print("æ˜¯å¦æŒæ¡çš„åˆ¤æ–­ï¼š" + response)
    return "å·²æŒæ¡" in response[-10:]


# Check if a teacher question is answered by student from the conversation history
def agent_check_is_question_answered(history):
    teacher_dialog, student_dialog = retrieve_dialog(history)
    start_time = time.time()
    if teacher_dialog == None or student_dialog == None:
        return None
    prompt = f'''ä»¥è¯·ä½ åŸºäºä»¥ä¸‹å¯¹è¯åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦æœ‰å¯¹è€å¸ˆçš„æé—®åšå‡ºå›ç­”ï¼š
## æ³¨æ„ï¼š
1. **è¯·åªè¾“å‡ºã€æ— å›ç­”ã€‘æˆ–ã€æœ‰å›ç­”ã€‘ï¼Œé™¤æ­¤ä»¥å¤–ä¸éœ€è¦ä»»ä½•é¢å¤–å†…å®¹**
2. å¦‚æœè€å¸ˆæ²¡æœ‰è¿›è¡Œæé—®ï¼Œè¯·ç›´æ¥è§†ä¸ºã€æ— å›ç­”ã€‘
3. åªéœ€è¦åˆ¤æ–­å­¦ç”Ÿæ˜¯å¦å›ç­”ï¼Œè¯·æ— è§†æ— è§†å…¶ä»–æ“ä½œ

### æ•™å¸ˆ
{teacher_dialog}
### å­¦ç”Ÿ
{student_dialog}
'''
    # Ask LLM for response
    response = get_llm_feedback(text=prompt)
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    logger.info(f'''====LLM's checking whether a question is answered====
ã€promptã€‘
{prompt}
ã€responseã€‘
{response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to}s  {len(response) / to} words/s")
    return "æœ‰å›ç­”" in response     # return whether the string "æœ‰å›ç­”" is present in the response (True or False)


# Error Tutor ä½ åªéœ€è¦ä»¥æ•™å¸ˆçš„å£å»è¾“å‡ºè¾…å¯¼ï¼Œä¸è¦è¾“å‡ºå…¶ä»–
def agent_error_tutor(question, history):
    teacher_dialog, student_dialog = retrieve_dialog(history)
    start_time = time.time()
    prompt = f'''ä»¥è¯·ä½ åŸºäºä»¥ä¸‹è€å¸ˆä¸å­¦ç”Ÿçš„å¯¹è¯ï¼Œå¯¹å­¦ç”Ÿçš„é”™è¯¯è¿›è¡Œè¾…å¯¼ï¼š
## æ³¨æ„ï¼š
1. **å­¦ç”Ÿçš„å›ç­”ä¸€å®šæ˜¯é”™è¯¯çš„ï¼è¯·ä½ å…ˆæ‰¾åˆ°å­¦ç”Ÿçš„é”™è¯¯ï¼**
2. **ä½ ä¸éœ€è¦è§£é¢˜ï¼Œåªè¦å¯¹å­¦ç”Ÿå½“å‰çš„é”™è¯¯å›ç­”è¿›è¡Œè¾…å¯¼ï¼Œè¯­è¨€è¦ç®€å•æ˜äº†**
3. ä½ æ˜¯ä¸€ä½å°å­¦æ•°å­¦æ•™å¸ˆï¼Œè€Œä¸æ˜¯AIåŠ©æ‰‹ï¼Œè¯·ä»¥**ç®€ç»ƒçš„è¯­è¨€è¾…å¯¼å°å­¦ç”Ÿï¼Œå¯¹è¯å¿…é¡»ç®€çŸ­æ˜äº†**

### é¢˜ç›®
{question}
### æ•™å¸ˆ
{teacher_dialog}
### å­¦ç”Ÿ
{student_dialog}
'''
    # Ask LLM for response
    response = get_llm_feedback(text=prompt)
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    logger.info(f'''====Error Tutor====
    ã€promptã€‘
    {prompt}
    ã€responseã€‘
    {response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to}s  {len(response) / to} words/s")
    # part = response.split("</think>\n\n", 1)
    # response = part[1]
    print("çº é”™åŠ©æ‰‹è¾“å‡ºï¼š" + response)
    return response

# Check if student answer to teacher question is correct from the conversation history
def agent_check_is_incorrect_answer(history, question):
    teacher_dialog, student_dialog = retrieve_dialog(history)
    start_time = time.time()
    # if "lcm" in st.session_state and st.session_state["lcm"]:
    # numbers = []
    # for element in st.session_state["lcm"]:
    #     numbers.append(element["denominator"])
    #
    # result = Function.LCM(numbers)
    # output = 'åˆ†æ¯' + ','.join(
    #     str(x) for x in numbers) + f"çš„æœ€å°å…¬å€æ•°æ˜¯{result}ï¼Œé€šåˆ†åçš„åˆ†æ¯ä¹Ÿåº”è¯¥æ˜¯{result}ï¼Œå…¶ä»–å›ç­”å‡é”™è¯¯ï¼" 1. **{output}**
    if teacher_dialog == None or student_dialog == None:
        return None
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    prompt = f'''ä»¥ä¸‹æ˜¯é¢˜ç›®å’Œæ•™å¸ˆä¸å­¦ç”Ÿçš„å¯¹è¯ï¼Œè¯·ä½ é€æ­¥æ€è€ƒï¼Œå†åˆ¤æ–­å­¦ç”Ÿçš„å›ç­”æ˜¯å¦æ­£ç¡®ï¼Œ**æœ€åç”Ÿæˆã€æ­£ç¡®ã€‘æˆ–ã€ä¸æ­£ç¡®ã€‘**
## æ³¨æ„ï¼š
1. è¯·ä»¥å°½é‡ç®€çŸ­çš„è¯è¯­æ¥è¿›è¡Œé€æ­¥æ€è€ƒï¼Œä¸è¦å¤ªè¿‡å†—é•¿
2. è¯·åœ¨ç»“å°¾ç”Ÿæˆã€æ­£ç¡®ã€‘æˆ–ã€ä¸æ­£ç¡®ã€‘
3. **ä½ ä¸éœ€è¦å»è§£é¢˜ï¼Œåªè¦åˆ¤æ–­å­¦ç”Ÿçš„å›ç­”å¯¹äºè€å¸ˆå½“å‰çš„æé—®æ˜¯å¦æ­£ç¡®**


### é¢˜ç›®
{question}
### æ•™å¸ˆ
{teacher_dialog}
### å­¦ç”Ÿ
{student_dialog}
'''
    response = get_llm_feedback(text=prompt)
    logger.info(f'''====LLM checking if an answer is incorrect====
    ã€promptã€‘
    {prompt}
    ã€responseã€‘
    {response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to}s  {len(response) / to} words/s")
    # part = response.split("</think>\n\n", 1)
    # response = part[1]
    print("æ˜¯å¦æ­£ç¡®çš„åˆ¤æ–­ï¼š" + response)
    return "ä¸æ­£ç¡®" in response[
                       -10:]  # return whether the string "ä¸æ­£ç¢º" is present in the last part of response (True or False)


def agent_tutor(user_input, question, explanation, history):
    system = SYSTEM
    history.append({"role": "user", "content": user_input})
    header = ""
    # Check if a valid conversation history (i.e. with a minimum length of 2)
    if len(history) >= 2:
        is_incorrect_answer = agent_check_is_incorrect_answer(history, question)  # Check if the answer is incorrect
        # if the answer is incorrect
        if is_incorrect_answer != None and is_incorrect_answer:
            result = agent_error_tutor(question, history)
            history.append({"role": "assistant", "content": result})

            return history, result

    start_time = time.time()
    header += f'''## é¢˜ç›®
{question}
## å½“å‰å¼•å¯¼æ­¥éª¤
{explanation}
## å­¦ç”Ÿè¾“å…¥\n
'''
    response = get_llm_response(user_input, history[:-1], header, system)
    history.append({"role": "assistant", "content": response})
    username = st.session_state.get("logged_user_name")
    logger = get_user_logger(username)
    logger.info(f'''====LLM response on step-by-step guidance====
    ã€promptã€‘
    {header + user_input}
    ã€responseã€‘
    {response}''')
    end_time = time.time()
    to = end_time - start_time
    logger.info(f"====TIME====  {to} s  {len(response) / to} words/s")
    # st.session_state.messages_with_error = ''
    return history, response



